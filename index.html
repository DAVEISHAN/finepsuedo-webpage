<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FinePseudo ECCV2024">
  <title>CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FinePseudo: Improving Pseudo-Labelling through Temporal-Alignablity for Semi-Supervised Fine-Grained Action Recognition</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://daveishan.github.io/">Ishan Dave</a>,</span>
            <span class="author-block">
              <a href="https://nayeemrizve.github.io/">Mamshad Nayeem Rizve</a>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Central Florida, Amazon Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <a href="https://arxiv.org/abs/2402.10478"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/DAVEISHAN/CodaMal"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              
              
              
            </div>
            


            

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Real-life applications of action recognition often require a fine-grained understanding of subtle movements, e.g., in sports analytics, user interactions in AR/VR, and surgical videos. Although fine-grained actions are more costly to annotate, existing semi-supervised action recognition has mainly focused on coarse-grained action recognition. Since fine-grained actions are more challenging due to the absence of scene bias, classifying these actions requires an understanding of action-phases. Hence, existing coarse-grained semi-supervised methods do not work effectively. In this work, we for the first time thoroughly investigate semi-supervised fine-grained action recognition (FGAR). We observe that alignment distances like dynamic time warping (DTW) provide a suitable action-phase-aware measure for comparing fine-grained actions, a concept previously unexploited in FGAR. However, since regular DTW distance is pairwise and assumes strict alignment between pairs, it is not directly suitable for classifying fine-grained actions. To utilize such alignment distances in a limited-label setting, we propose an Alignability-Verification-based Metric learning technique to effectively discriminate between fine-grained action pairs. Our learnable alignability score provides a better phase-aware measure, which we use to refine the pseudo-labels of the primary video encoder. Our collaborative pseudo-labeling-based framework 'FinePseudo' significantly outperforms prior methods on four fine-grained action recognition datasets: Diving48, FineGym99, FineGym288, and FineDiving, and shows improvement on existing coarse-grained datasets: Kinetics400 and Something-SomethingV2. We also demonstrate the robustness of our collaborative pseudo-labeling in handling novel unlabeled classes in open-world semi-supervised setups.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


  

    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <p>
              Schematic diagram of our end-to-end method for malaria detection. (a) During training, an HCM image (Xih) and its annotations (Yih) are paired with the corresponding unannotated LCM image (Xil) that is captured from the same field-of-view, instance i.
              The HCM image (Xih) is utilized to train the backbone (f) and its detection head (h) using the standard object detection losses (LOD). For domain adaptation, both Xih and Xil are processed through the same backbone (f) and non-linear 
              projection head (g). The HCM representation, Zih, is encouraged to maximize similarity with the corresponding LCM instance (Zil) (indicated by the green arrow), while minimizing similarity with the representations of other LCM and HCM image
              instances (j,k) i.e. instances captured from different FOV or different blood smear (highlighted by the red arrow). (b) Once trained, the model receives LCM images and predicts the location and life-stage of the malaria parasite as its final output.
            </p>
            <img src="./static/images/img1.jpg"
                class="interpolation-image"
                alt="Interpolate start reference image."/>
          </div>
        </div>
      </div>

      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Results</h2>
            <div class="content has-text-justified">
              <p>
                Our method, CodaMal, significantly outperforms prior best results by 42.93% and 44.08% on 1000× and 400× of FOV in terms of mAP and sets a new
                state-of-art. Also, it is worth noting that our model utilizes only 21.2M parameters which is slightly less than ResNet50 (23M) used by prior work.
                Apart, from mAP metric, our method achieves a precision of 0.8761 and Recall of 0.4571, on 1000× magnification FOV setting.
              </p>
              <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-steve">
                  <h3 class="image-title">1000x</h3>
                  <img src="./static/images/1000x.png"
                  class="interpolation-image"
                  alt="Interpolate start reference image."/>
                </div>
                <div class="item item-chair-tp">
                  <h3 class="image-title">400x</h3>
                  <img src="./static/images/400x.png"
                  class="interpolation-image"
                  alt="Interpolate start reference image."/>
                </div>
                <div class="item item-shiba">
                  <h3 class="image-title">100x</h3>
                  <img src="./static/images/100x.png"
                  class="interpolation-image"
                  alt="Interpolate start reference image."/>
                </div>
                <div class="item item-steve">
                  <h3 class="image-title">1000x</h3>
                  <img src="./static/images/1000x.png"
                  class="interpolation-image"
                  alt="Interpolate start reference image."/>
                </div>
                <div class="item item-chair-tp">
                  <h3 class="image-title">400x</h3>
                  <img src="./static/images/400x.png"
                  class="interpolation-image"
                  alt="Interpolate start reference image."/>
                </div>
                <div class="item item-shiba">
                  <h3 class="image-title">100x</h3>
                  <img src="./static/images/100x.png"
                  class="interpolation-image"
                  alt="Interpolate start reference image."/>
                </div>
              </div>
              <p></p>
              <table>
                <thead>
                  <tr>
                    <th>Method</th>
                    <th>End-to-End Training?</th>
                    <th>Number of Params (M)</th>
                    <th>Inf. time (ms)</th>
                    <th colspan="2">HCM → LCM</th>
                  </tr>
                  <tr>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th></th>
                    <th>1000x</th>
                    <th>400x</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Chen et al.. <span class="gray-text">CVPR'18</span></td>
                    <td>✓</td>
                    <td>43.7</td>
                    <td>184</td>
                    <td>17.6</td>
                    <td>21.5</td>
                  </tr>
                  <tr>
                    <td>Saito et al. <span class="gray-text">CVPR'19</span></td>
                    <td>✓</td>
                    <td>43.7</td>
                    <td>184</td>
                    <td>24.8</td>
                    <td>21.4</td>
                  </tr>
                  <tr>
                    <td>Xu et al. <span class="gray-text">CVPR'20</span></td>
                    <td>✓</td>
                    <td>43.7</td>
                    <td>184</td>
                    <td>15.5</td>
                    <td>21.6</td>
                  </tr>
                  <tr>
                    <td>Faster RCNN</td>
                    <td>✓</td>
                    <td>43.7</td>
                    <td>184</td>
                    <td>17.1</td>
                    <td>26.7</td>
                  </tr>
                  <tr>
                    <td>Faster RCNN + Synthetic LCM Tuning</td>
                    <td>✗</td>
                    <td>43.7</td>
                    <td>184</td>
                    <td>33.3</td>
                    <td>31.8</td>
                  </tr>
                  <tr>
                    <td>Faster RCNN + Synthetic LCM Tuning + Ranking Loss</td>
                    <td>✗</td>
                    <td>43.7</td>
                    <td>184</td>
                    <td>35.7</td>
                    <td>32.4</td>
                  </tr>
                  <tr>
                    <td>Faster RCNN + Synthetic LCM Tuning + Triplet Loss</td>
                    <td>✗</td>
                    <td>43.7</td>
                    <td>184</td>
                    <td>37.2</td>
                    <td>32.2</td>
                  </tr>
                  <tr>
                    <td>Sultani et al. <span class="gray-text">CVPR'22</span></td>
                    <td>✓</td>
                    <td>43.7</td>
                    <td>184</td>
                    <td><u><span class="blue-text">37.5</span></u></td>
                    <td><u><span class="blue-text">33.8</span></u></td>
                  </tr>
                  <tr class="highlight">
                    <td><strong>CodaMal (Ours)</strong></td>
                    <td><strong>✓</strong></td>
                    <td><strong>21.2</strong></td>
                    <td><strong>8.9</strong></td>
                    <td><strong><span class="red-text">53.6</span></strong></td>
                    <td><strong><span class="red-text">48.7</span></strong></td>
                  </tr>
                </tbody>
              </table>
              <p>
                The performance metric is mAP @ 0.5 IoU. 1000× and 400× repre-
                sent the magnification FOV of LCM images. Highlighted <strong><span class="red-text">red</span></strong> shows the best results and <u><span class="blue-text">blue</span></u> shows second best results.
              </p>

            </div>
          </div>
        </div>

        <section class="section">
          <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <h2 class="title is-3">Conclusion</h2>
                <div class="content has-text-justified">
                  <p>
                    In conclusion, our study presents a novel end-to-end train-able method for the more practical setting of deep learning
                    learning-based malaria parasite detection: training on HCM and testing on LCM images. The incorporation of the do-
                    main adaptive contrastive loss bridges the domain gap by pro-moting invariance between HCM and LCM domains. Our
                    method, CodaMal, substantially outperforms the prior best methods up to 44% and sets a new state-of-the-art. We are
                    planning to open-source our implementation code upon the acceptance of this work. These findings not only highlight the potential of our
                    method to enhance the early and accurate diagnosis of malaria but also emphasize the importance of domain adaptation in
                    improving the generalization capabilities of deep learning-based models for microscopy applications. Future research
                    directions may explore the application of our method to other microscopy-based diagnostic tasks, further refining
                    the method and assessing its impact on a broader range of diseases and detection scenarios.
                  </p>
                </div>
              </div>
            </div>
      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>
           @article{dave2024codamal,
              title={CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes},
              author={Dave, Ishan Rajendrakumar and de Blegiers, Tristan and Chen, Chen and Shah, Mubarak},
              journal={arXiv preprint arXiv:2402.10478},
              year={2024}
            }
          </code></pre>
        </div>
      </section>

</body>
</html>
